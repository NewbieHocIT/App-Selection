import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from PIL import Image, ImageOps
import pandas as pd
import os
import mlflow
from datetime import datetime
from sklearn.datasets import fetch_openml
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score
# Load d·ªØ li·ªáu MNIST
def load_mnist():
    mnist = fetch_openml('mnist_784', version=1, as_frame=False)
    X, y = mnist.data, mnist.target.astype(int)
    return X, y

def data():
    st.header("MNIST Dataset")
    st.write("""
      **MNIST** l√† m·ªôt trong nh·ªØng b·ªô d·ªØ li·ªáu n·ªïi ti·∫øng v√† ph·ªï bi·∫øn nh·∫•t trong c·ªông ƒë·ªìng h·ªçc m√°y, 
      ƒë·∫∑c bi·ªát l√† trong c√°c nghi√™n c·ª©u v·ªÅ nh·∫≠n di·ªán m·∫´u v√† ph√¢n lo·∫°i h√¨nh ·∫£nh.
  
      - B·ªô d·ªØ li·ªáu bao g·ªìm t·ªïng c·ªông **70.000 ·∫£nh ch·ªØ s·ªë vi·∫øt tay** t·ª´ **0** ƒë·∫øn **9**, 
        m·ªói ·∫£nh c√≥ k√≠ch th∆∞·ªõc **28 x 28 pixel**.
      - Chia th√†nh:
        - **Training set**: 60.000 ·∫£nh ƒë·ªÉ hu·∫•n luy·ªán.
        - **Test set**: 10.000 ·∫£nh ƒë·ªÉ ki·ªÉm tra.
      - M·ªói h√¨nh ·∫£nh l√† m·ªôt ch·ªØ s·ªë vi·∫øt tay, ƒë∆∞·ª£c chu·∫©n h√≥a v√† chuy·ªÉn th√†nh d·∫°ng grayscale (ƒëen tr·∫Øng).
  
      D·ªØ li·ªáu n√†y ƒë∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i ƒë·ªÉ x√¢y d·ª±ng c√°c m√¥ h√¨nh nh·∫≠n di·ªán ch·ªØ s·ªë.
      """)

    st.subheader("M·ªôt s·ªë h√¨nh ·∫£nh t·ª´ MNIST Dataset")
    st.image("mnit.png", caption="M·ªôt s·ªë h√¨nh ·∫£nh t·ª´ MNIST Dataset", use_container_width=True)

    st.subheader("·ª®ng d·ª•ng th·ª±c t·∫ø c·ªßa MNIST")
    st.write("""
      B·ªô d·ªØ li·ªáu MNIST ƒë√£ ƒë∆∞·ª£c s·ª≠ d·ª•ng trong nhi·ªÅu ·ª©ng d·ª•ng nh·∫≠n d·∫°ng ch·ªØ s·ªë vi·∫øt tay, ch·∫≥ng h·∫°n nh∆∞:
      - Nh·∫≠n di·ªán s·ªë tr√™n c√°c ho√° ƒë∆°n thanh to√°n, bi√™n lai c·ª≠a h√†ng.
      - X·ª≠ l√Ω ch·ªØ s·ªë tr√™n c√°c b∆∞u ki·ªán g·ª≠i qua b∆∞u ƒëi·ªán.
      - ·ª®ng d·ª•ng trong c√°c h·ªá th·ªëng nh·∫≠n di·ªán t√†i li·ªáu t·ª± ƒë·ªông.
    """)

    st.subheader("V√≠ d·ª• v·ªÅ c√°c m√¥ h√¨nh h·ªçc m√°y v·ªõi MNIST")
    st.write("""
      C√°c m√¥ h√¨nh h·ªçc m√°y ph·ªï bi·∫øn ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi b·ªô d·ªØ li·ªáu MNIST bao g·ªìm:
      - **Logistic Regression**
      - **Decision Trees**
      - **K-Nearest Neighbors (KNN)**
      - **Support Vector Machines (SVM)**
      - **Convolutional Neural Networks (CNNs)**
    """)


def reduce_dimensions(X, method='PCA', n_components=2):
    if method == 'PCA':
        reducer = PCA(n_components=n_components)
    elif method == 't-SNE':
        reducer = TSNE(n_components=n_components, perplexity=30, n_iter=300)
    else:
        raise ValueError("Ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu kh√¥ng h·ª£p l·ªá. Ch·ªçn 'PCA' ho·∫∑c 't-SNE'.")
    
    X_reduced = reducer.fit_transform(X)
    return X_reduced
def split_data():
    st.title("üìå Chia d·ªØ li·ªáu Train/Validation/Test")

    # T·∫£i d·ªØ li·ªáu MNIST
    X, y = load_mnist()
    total_samples = X.shape[0]

    num_samples = st.slider(
        "Ch·ªçn s·ªë l∆∞·ª£ng ·∫£nh ƒë·ªÉ train (‚ö†Ô∏è S·ªë l∆∞·ª£ng l·ªõn s·∫Ω l√¢u h∆°n):", 
        1000, total_samples, 10000, 
        key="clustering_num_samples_slider"  # Th√™m key duy nh·∫•t
    )
    st.session_state.total_samples = num_samples

    # Ch·ªçn t·ªâ l·ªá cho t·∫≠p train v√† validation
    train_ratio = st.slider(
        "üìå Ch·ªçn % d·ªØ li·ªáu Train", 
        50, 90, 70, 
        key="clustering_train_ratio_slider"  # Th√™m key duy nh·∫•t
    )
    val_ratio = st.slider(
        "üìå Ch·ªçn % d·ªØ li·ªáu Validation", 
        10, 40, 15, 
        key="clustering_val_ratio_slider"  # Th√™m key duy nh·∫•t
    )
    test_ratio = 100 - train_ratio - val_ratio

    if test_ratio < 10:
        st.warning("‚ö†Ô∏è T·ªâ l·ªá d·ªØ li·ªáu Test qu√° th·∫•p (d∆∞·ªõi 10%). H√£y ƒëi·ªÅu ch·ªânh l·∫°i t·ªâ l·ªá Train v√† Validation.")
    else:
        st.write(f"üìå **T·ª∑ l·ªá ph√¢n chia:** Train={train_ratio}%, Validation={val_ratio}%, Test={test_ratio}%")
    
    if st.button("‚úÖ X√°c nh·∫≠n & L∆∞u", key="clustering_confirm_button"):  # Th√™m key duy nh·∫•t
        X_selected, _, y_selected, _ = train_test_split(X, y, train_size=num_samples, stratify=y, random_state=42)

        stratify_option = y_selected if len(np.unique(y_selected)) > 1 else None
        X_train_full, X_test, y_train_full, y_test = train_test_split(
            X_selected, y_selected, test_size=test_ratio/100, stratify=stratify_option, random_state=42
        )

        stratify_option = y_train_full if len(np.unique(y_train_full)) > 1 else None
        X_train, X_val, y_train, y_val = train_test_split(
            X_train_full, y_train_full, test_size=val_ratio / (train_ratio + val_ratio),
            stratify=stratify_option, random_state=42
        )

        st.session_state.X_train = X_train
        st.session_state.X_val = X_val
        st.session_state.X_test = X_test
        st.session_state.y_train = y_train
        st.session_state.y_val = y_val
        st.session_state.y_test = y_test
        st.session_state.test_size = X_test.shape[0]
        st.session_state.val_size = X_val.shape[0]
        st.session_state.train_size = X_train.shape[0]
    
        summary_df = pd.DataFrame({
            "T·∫≠p d·ªØ li·ªáu": ["Train", "Validation", "Test"],
            "S·ªë l∆∞·ª£ng m·∫´u": [X_train.shape[0], X_val.shape[0], X_test.shape[0]]
        })
        st.success("‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c chia th√†nh c√¥ng!")
        st.table(summary_df)
        
def mlflow_input():
    DAGSHUB_MLFLOW_URI = "https://dagshub.com/NewbieHocIT/MocMayvsPython.mlflow"
    st.session_state['mlflow_url'] = DAGSHUB_MLFLOW_URI
    mlflow.set_tracking_uri(DAGSHUB_MLFLOW_URI)

    os.environ["MLFLOW_TRACKING_USERNAME"] = "NewbieHocIT"
    os.environ["MLFLOW_TRACKING_PASSWORD"] = "681dda9a41f9271a144aa94fa8624153a3c95696"

    mlflow.set_experiment("Clustering")

def train_model(model, X_train_reduced, y_train, X_test_reduced, y_test, model_choice, reduction_method, n_components, n_clusters=None, eps=None, min_samples=None):
    """
    H√†m hu·∫•n luy·ªán m√¥ h√¨nh v√† log k·∫øt qu·∫£ v√†o MLflow.
    """
    with mlflow.start_run(run_name=f"Train_{st.session_state['run_name']}"):
        # Log c√°c tham s·ªë
        mlflow.log_param("test_size", st.session_state.test_size)
        mlflow.log_param("val_size", st.session_state.val_size)
        mlflow.log_param("train_size", st.session_state.train_size)
        mlflow.log_param("num_samples", st.session_state.total_samples)
        mlflow.log_param("reduction_method", reduction_method)
        mlflow.log_param("n_components", n_components)

        if model_choice == "K-means":
            mlflow.log_param("n_clusters", n_clusters)
        elif model_choice == "DBSCAN":
            mlflow.log_param("eps", eps)
            mlflow.log_param("min_samples", min_samples)

        st.write("‚è≥ ƒêang hu·∫•n luy·ªán m√¥ h√¨nh...")
        model.fit(X_train_reduced)
        labels = model.labels_

        # T√≠nh to√°n silhouette score
        if len(np.unique(labels)) > 1:
            silhouette_avg = silhouette_score(X_train_reduced, labels)
            st.success(f"üìä **Silhouette Score**: {silhouette_avg:.4f}")
            mlflow.log_metric("silhouette_score", silhouette_avg)
        else:
            st.warning("‚ö† Kh√¥ng th·ªÉ t√≠nh silhouette score v√¨ ch·ªâ c√≥ m·ªôt c·ª•m.")

        # L∆∞u m√¥ h√¨nh v√†o session_state
        if "models" not in st.session_state:
            st.session_state["models"] = []

        model_name = model_choice.lower().replace(" ", "_")
        if model_choice == "DBSCAN":
            model_name += f"_eps{eps}_min_samples{min_samples}"
        elif model_choice == "K-means":
            model_name += f"_n_clusters{n_clusters}"

        existing_model = next((item for item in st.session_state["models"] if item["name"] == model_name), None)

        if existing_model:
            count = 1
            new_model_name = f"{model_name}_{count}"
            while any(item["name"] == new_model_name for item in st.session_state["models"]):
                count += 1
                new_model_name = f"{model_name}_{count}"
            model_name = new_model_name
            st.warning(f"‚ö†Ô∏è M√¥ h√¨nh ƒë∆∞·ª£c l∆∞u v·ªõi t√™n: {model_name}")

        st.session_state["models"].append({"name": model_name, "model": model})
        st.write(f"üîπ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u v·ªõi t√™n: {model_name}")
        st.write(f"T·ªïng s·ªë m√¥ h√¨nh hi·ªán t·∫°i: {len(st.session_state['models'])}")

        st.write("üìã Danh s√°ch c√°c m√¥ h√¨nh ƒë√£ l∆∞u:")
        model_names = [model["name"] for model in st.session_state["models"]]
        st.write(", ".join(model_names))

        st.success(f"‚úÖ ƒê√£ log d·ªØ li·ªáu cho **Train_{st.session_state['run_name']}**!")
        st.markdown(f"üîó [Truy c·∫≠p MLflow UI]({st.session_state['mlflow_url']})")


def train():
    mlflow_input()
    if "X_train" in st.session_state:
        X_train = st.session_state.X_train 
        X_val = st.session_state.X_val 
        X_test = st.session_state.X_test 
        y_train = st.session_state.y_train 
        y_val = st.session_state.y_val 
        y_test = st.session_state.y_test 
    else:
        st.error("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu! H√£y chia d·ªØ li·ªáu tr∆∞·ªõc.")
        return

    X_train = X_train.reshape(-1, 28 * 28) / 255.0
    X_test = X_test.reshape(-1, 28 * 28) / 255.0

    st.header("‚öôÔ∏è Ch·ªçn m√¥ h√¨nh & Hu·∫•n luy·ªán")

    model_choice = st.selectbox(
        "Ch·ªçn m√¥ h√¨nh:", 
        ["K-means", "DBSCAN"], 
        key="clustering_model_choice_selectbox"  # Th√™m key duy nh·∫•t
    )

    # Gi·∫£m chi·ªÅu d·ªØ li·ªáu
    reduction_method = st.selectbox(
        "Ch·ªçn ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu d·ªØ li·ªáu:", 
        ["PCA", "t-SNE"], 
        key="clustering_reduction_method_selectbox"  # Th√™m key duy nh·∫•t
    )
    n_components = st.slider(
        "S·ªë chi·ªÅu sau khi gi·∫£m:", 
        2, 
        50 if reduction_method == "PCA" else 3,  # Gi·ªõi h·∫°n t-SNE t·ªëi ƒëa l√† 3
        2,
        key="clustering_n_components_slider"  # Th√™m key duy nh·∫•t
    )

    # L∆∞u v√†o session_state
    st.session_state.reduction_method = reduction_method
    st.session_state.n_components = n_components

    X_train_reduced = reduce_dimensions(X_train, method=reduction_method, n_components=n_components)
    X_test_reduced = reduce_dimensions(X_test, method=reduction_method, n_components=n_components)

    if model_choice == "K-means":
        st.markdown("""
        - **K-means** l√† m·ªôt thu·∫≠t to√°n ph√¢n c·ª•m d·ª±a tr√™n kho·∫£ng c√°ch gi·ªØa c√°c ƒëi·ªÉm d·ªØ li·ªáu.
        - **Tham s·ªë c·∫ßn ch·ªçn:**  
            - **n_clusters**: S·ªë l∆∞·ª£ng c·ª•m.  
        """)
        n_clusters = st.slider(
            "n_clusters", 
            2, 20, 10, 
            key="clustering_n_clusters_slider"  # Th√™m key duy nh·∫•t
        )
        model = KMeans(n_clusters=n_clusters)

    elif model_choice == "DBSCAN":
        st.markdown("""
        - **DBSCAN** l√† m·ªôt thu·∫≠t to√°n ph√¢n c·ª•m d·ª±a tr√™n m·∫≠t ƒë·ªô.
        """)
        eps = st.slider(
            "eps (Kho·∫£ng c√°ch t·ªëi ƒëa gi·ªØa hai ƒëi·ªÉm ƒë·ªÉ coi l√† l√¢n c·∫≠n)", 
            0.1, 1.0, 0.5, 
            key="clustering_eps_slider"  # Th√™m key duy nh·∫•t
        )
        min_samples = st.slider(
            "min_samples (S·ªë l∆∞·ª£ng ƒëi·ªÉm t·ªëi thi·ªÉu trong m·ªôt l√¢n c·∫≠n)", 
            1, 20, 5, 
            key="clustering_min_samples_slider"  # Th√™m key duy nh·∫•t
        )
        model = DBSCAN(eps=eps, min_samples=min_samples)

    if st.button("Hu·∫•n luy·ªán m√¥ h√¨nh", key="clustering_train_button"):  # Th√™m key duy nh·∫•t
        st.session_state["run_name"] = "training_run_1"  # T·∫°o gi√° tr·ªã cho run_name

        # G·ªçi h√†m hu·∫•n luy·ªán m√¥ h√¨nh
        train_model(
            model=model,
            X_train_reduced=X_train_reduced,
            y_train=y_train,
            X_test_reduced=X_test_reduced,
            y_test=y_test,
            model_choice=model_choice,
            reduction_method=reduction_method,
            n_components=n_components,
            n_clusters=n_clusters if model_choice == "K-means" else None,
            eps=eps if model_choice == "DBSCAN" else None,
            min_samples=min_samples if model_choice == "DBSCAN" else None
        )


def du_doan():
    st.title("üî¢ D·ª± ƒëo√°n ch·ªØ s·ªë vi·∫øt tay")

    if "models" not in st.session_state or not st.session_state["models"]:
        st.error("‚ö†Ô∏è Ch∆∞a c√≥ m√¥ h√¨nh n√†o ƒë∆∞·ª£c hu·∫•n luy·ªán. H√£y hu·∫•n luy·ªán m√¥ h√¨nh tr∆∞·ªõc.")
        return

    model_names = [model["name"] for model in st.session_state["models"]]
    selected_model_name = st.selectbox(
        "üîç Ch·ªçn m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán:", 
        model_names, 
        key="clustering_model_selectbox"  # Th√™m key duy nh·∫•t
    )
    selected_model = next(model["model"] for model in st.session_state["models"] if model["name"] == selected_model_name)

    uploaded_file = st.file_uploader(
        "üì§ T·∫£i l√™n ·∫£nh ch·ªØ s·ªë vi·∫øt tay (28x28 pixel)", 
        type=["png", "jpg", "jpeg"], 
        key="clustering_file_uploader"  # Th√™m key duy nh·∫•t
    )

    if uploaded_file is not None:
        image = Image.open(uploaded_file).convert("L")
        image = ImageOps.invert(image)
        image = image.resize((28, 28))
        st.image(image, caption="·∫¢nh ƒë√£ t·∫£i l√™n", use_column_width=True)

        img_array = np.array(image).reshape(1, -1) / 255.0
        prediction = selected_model.predict(img_array)
        st.success(f"üî¢ D·ª± ƒëo√°n: **{prediction[0]}**")

def show_experiment_selector():
    st.title("üìä MLflow Experiments - DAGsHub")

    experiment_name = "Clustering"
    
    experiments = mlflow.search_experiments()
    selected_experiment = next((exp for exp in experiments if exp.name == experiment_name), None)

    if not selected_experiment:
        st.error(f"‚ùå Experiment '{experiment_name}' kh√¥ng t·ªìn t·∫°i!")
        return

    st.subheader(f"üìå Experiment: {experiment_name}")
    st.write(f"**Experiment ID:** {selected_experiment.experiment_id}")
    st.write(f"**Tr·∫°ng th√°i:** {'Active' if selected_experiment.lifecycle_stage == 'active' else 'Deleted'}")
    st.write(f"**V·ªã tr√≠ l∆∞u tr·ªØ:** {selected_experiment.artifact_location}")

    runs = mlflow.search_runs(experiment_ids=[selected_experiment.experiment_id])

    if runs.empty:
        st.warning("‚ö† Kh√¥ng c√≥ runs n√†o trong experiment n√†y.")
        return

    st.write("### üèÉ‚Äç‚ôÇÔ∏è C√°c Runs g·∫ßn ƒë√¢y:")

    run_info = []
    for _, run in runs.iterrows():
        run_id = run["run_id"]
        run_params = mlflow.get_run(run_id).data.params
        run_name = run_params.get("run_name", f"Run {run_id[:8]}")
        run_info.append((run_name, run_id))

    run_name_to_id = dict(run_info)
    run_names = list(run_name_to_id.keys())

    # Th√™m key duy nh·∫•t cho selectbox
    selected_run_name = st.selectbox(
        "üîç Ch·ªçn m·ªôt run:", 
        run_names, 
        key="clustering_experiment_selectbox"  # Th√™m key duy nh·∫•t
    )
    selected_run_id = run_name_to_id[selected_run_name]

    selected_run = mlflow.get_run(selected_run_id)

    if selected_run:
        st.subheader(f"üìå Th√¥ng tin Run: {selected_run_name}")
        st.write(f"**Run ID:** {selected_run_id}")
        st.write(f"**Tr·∫°ng th√°i:** {selected_run.info.status}")
        start_time_ms = selected_run.info.start_time

        if start_time_ms:
            start_time = datetime.fromtimestamp(start_time_ms / 1000).strftime("%Y-%m-%d %H:%M:%S")
        else:
            start_time = "Kh√¥ng c√≥ th√¥ng tin"

        st.write(f"**Th·ªùi gian ch·∫°y:** {start_time}")

        params = selected_run.data.params
        metrics = selected_run.data.metrics

        if params:
            st.write("### ‚öôÔ∏è Parameters:")
            st.json(params)

        if metrics:
            st.write("### üìä Metrics:")
            st.json(metrics)

        dataset_path = f"{selected_experiment.artifact_location}/{selected_run_id}/artifacts/dataset.csv"
        st.write("### üìÇ Dataset:")
        st.write(f"üì• [T·∫£i dataset]({dataset_path})")
    else:
        st.warning("‚ö† Kh√¥ng t√¨m th·∫•y th√¥ng tin cho run n√†y.")


def Clusterting():
    st.title(" MNIST Clustering App")

    tab1, tab2, tab3, tab4 = st.tabs(["üìò Data", "‚öôÔ∏è Hu·∫•n luy·ªán", "üî¢ D·ª± ƒëo√°n", "üî•Mlflow"])

    with tab1:
        data()
        
    with tab2:
        split_data()
        train()
        
    with tab3:
        du_doan()   
    with tab4:
        show_experiment_selector()  

if __name__ == "__main__":
    Clusterting()


def data():
    st.header("MNIST Dataset")
    st.write("""
      **MNIST** l√† m·ªôt trong nh·ªØng b·ªô d·ªØ li·ªáu n·ªïi ti·∫øng v√† ph·ªï bi·∫øn nh·∫•t trong c·ªông ƒë·ªìng h·ªçc m√°y, 
      ƒë·∫∑c bi·ªát l√† trong c√°c nghi√™n c·ª©u v·ªÅ nh·∫≠n di·ªán m·∫´u v√† ph√¢n lo·∫°i h√¨nh ·∫£nh.
  
      - B·ªô d·ªØ li·ªáu bao g·ªìm t·ªïng c·ªông **70.000 ·∫£nh ch·ªØ s·ªë vi·∫øt tay** t·ª´ **0** ƒë·∫øn **9**, 
        m·ªói ·∫£nh c√≥ k√≠ch th∆∞·ªõc **28 x 28 pixel**.
      - Chia th√†nh:
        - **Training set**: 60.000 ·∫£nh ƒë·ªÉ hu·∫•n luy·ªán.
        - **Test set**: 10.000 ·∫£nh ƒë·ªÉ ki·ªÉm tra.
      - M·ªói h√¨nh ·∫£nh l√† m·ªôt ch·ªØ s·ªë vi·∫øt tay, ƒë∆∞·ª£c chu·∫©n h√≥a v√† chuy·ªÉn th√†nh d·∫°ng grayscale (ƒëen tr·∫Øng).
  
      D·ªØ li·ªáu n√†y ƒë∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i ƒë·ªÉ x√¢y d·ª±ng c√°c m√¥ h√¨nh nh·∫≠n di·ªán ch·ªØ s·ªë.
      """)

    st.subheader("M·ªôt s·ªë h√¨nh ·∫£nh t·ª´ MNIST Dataset")
    st.image("mnit.png", caption="M·ªôt s·ªë h√¨nh ·∫£nh t·ª´ MNIST Dataset", use_container_width=True)

    st.subheader("·ª®ng d·ª•ng th·ª±c t·∫ø c·ªßa MNIST")
    st.write("""
      B·ªô d·ªØ li·ªáu MNIST ƒë√£ ƒë∆∞·ª£c s·ª≠ d·ª•ng trong nhi·ªÅu ·ª©ng d·ª•ng nh·∫≠n d·∫°ng ch·ªØ s·ªë vi·∫øt tay, ch·∫≥ng h·∫°n nh∆∞:
      - Nh·∫≠n di·ªán s·ªë tr√™n c√°c ho√° ƒë∆°n thanh to√°n, bi√™n lai c·ª≠a h√†ng.
      - X·ª≠ l√Ω ch·ªØ s·ªë tr√™n c√°c b∆∞u ki·ªán g·ª≠i qua b∆∞u ƒëi·ªán.
      - ·ª®ng d·ª•ng trong c√°c h·ªá th·ªëng nh·∫≠n di·ªán t√†i li·ªáu t·ª± ƒë·ªông.
    """)

    st.subheader("V√≠ d·ª• v·ªÅ c√°c m√¥ h√¨nh h·ªçc m√°y v·ªõi MNIST")
    st.write("""
      C√°c m√¥ h√¨nh h·ªçc m√°y ph·ªï bi·∫øn ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi b·ªô d·ªØ li·ªáu MNIST bao g·ªìm:
      - **Logistic Regression**
      - **Decision Trees**
      - **K-Nearest Neighbors (KNN)**
      - **Support Vector Machines (SVM)**
      - **Convolutional Neural Networks (CNNs)**
    """)


def reduce_dimensions(X, method='PCA', n_components=2):
    if method == 'PCA':
        reducer = PCA(n_components=n_components)
    elif method == 't-SNE':
        reducer = TSNE(n_components=n_components, perplexity=30, n_iter=300)
    else:
        raise ValueError("Ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu kh√¥ng h·ª£p l·ªá. Ch·ªçn 'PCA' ho·∫∑c 't-SNE'.")
    
    X_reduced = reducer.fit_transform(X)
    return X_reduced
def split_data():
    st.title("üìå Chia d·ªØ li·ªáu Train/Validation/Test")

    # T·∫£i d·ªØ li·ªáu MNIST
    X, y = load_mnist()
    total_samples = X.shape[0]

    num_samples = st.slider(
        "Ch·ªçn s·ªë l∆∞·ª£ng ·∫£nh ƒë·ªÉ train (‚ö†Ô∏è S·ªë l∆∞·ª£ng l·ªõn s·∫Ω l√¢u h∆°n):", 
        1000, total_samples, 10000, 
        key="clustering_num_samples_slider"  # Th√™m key duy nh·∫•t
    )
    st.session_state.total_samples = num_samples

    # Ch·ªçn t·ªâ l·ªá cho t·∫≠p train v√† validation
    train_ratio = st.slider(
        "üìå Ch·ªçn % d·ªØ li·ªáu Train", 
        50, 90, 70, 
        key="clustering_train_ratio_slider"  # Th√™m key duy nh·∫•t
    )
    val_ratio = st.slider(
        "üìå Ch·ªçn % d·ªØ li·ªáu Validation", 
        10, 40, 15, 
        key="clustering_val_ratio_slider"  # Th√™m key duy nh·∫•t
    )
    test_ratio = 100 - train_ratio - val_ratio

    if test_ratio < 10:
        st.warning("‚ö†Ô∏è T·ªâ l·ªá d·ªØ li·ªáu Test qu√° th·∫•p (d∆∞·ªõi 10%). H√£y ƒëi·ªÅu ch·ªânh l·∫°i t·ªâ l·ªá Train v√† Validation.")
    else:
        st.write(f"üìå **T·ª∑ l·ªá ph√¢n chia:** Train={train_ratio}%, Validation={val_ratio}%, Test={test_ratio}%")
    
    if st.button("‚úÖ X√°c nh·∫≠n & L∆∞u", key="clustering_confirm_button"):  # Th√™m key duy nh·∫•t
        X_selected, _, y_selected, _ = train_test_split(X, y, train_size=num_samples, stratify=y, random_state=42)

        stratify_option = y_selected if len(np.unique(y_selected)) > 1 else None
        X_train_full, X_test, y_train_full, y_test = train_test_split(
            X_selected, y_selected, test_size=test_ratio/100, stratify=stratify_option, random_state=42
        )

        stratify_option = y_train_full if len(np.unique(y_train_full)) > 1 else None
        X_train, X_val, y_train, y_val = train_test_split(
            X_train_full, y_train_full, test_size=val_ratio / (train_ratio + val_ratio),
            stratify=stratify_option, random_state=42
        )

        st.session_state.X_train = X_train
        st.session_state.X_val = X_val
        st.session_state.X_test = X_test
        st.session_state.y_train = y_train
        st.session_state.y_val = y_val
        st.session_state.y_test = y_test
        st.session_state.test_size = X_test.shape[0]
        st.session_state.val_size = X_val.shape[0]
        st.session_state.train_size = X_train.shape[0]
    
        summary_df = pd.DataFrame({
            "T·∫≠p d·ªØ li·ªáu": ["Train", "Validation", "Test"],
            "S·ªë l∆∞·ª£ng m·∫´u": [X_train.shape[0], X_val.shape[0], X_test.shape[0]]
        })
        st.success("‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c chia th√†nh c√¥ng!")
        st.table(summary_df)
        
def mlflow_input():
    DAGSHUB_MLFLOW_URI = "https://dagshub.com/NewbieHocIT/MocMayvsPython.mlflow"
    st.session_state['mlflow_url'] = DAGSHUB_MLFLOW_URI
    mlflow.set_tracking_uri(DAGSHUB_MLFLOW_URI)

    os.environ["MLFLOW_TRACKING_USERNAME"] = "NewbieHocIT"
    os.environ["MLFLOW_TRACKING_PASSWORD"] = "681dda9a41f9271a144aa94fa8624153a3c95696"

    mlflow.set_experiment("Clustering")

def train_model(model, X_train_reduced, y_train, X_test_reduced, y_test, model_choice, reduction_method, n_components, n_clusters=None, eps=None, min_samples=None):
    """
    H√†m hu·∫•n luy·ªán m√¥ h√¨nh v√† log k·∫øt qu·∫£ v√†o MLflow.
    """
    with mlflow.start_run(run_name=f"Train_{st.session_state['run_name']}"):
        # Log c√°c tham s·ªë
        mlflow.log_param("test_size", st.session_state.test_size)
        mlflow.log_param("val_size", st.session_state.val_size)
        mlflow.log_param("train_size", st.session_state.train_size)
        mlflow.log_param("num_samples", st.session_state.total_samples)
        mlflow.log_param("reduction_method", reduction_method)
        mlflow.log_param("n_components", n_components)

        if model_choice == "K-means":
            mlflow.log_param("n_clusters", n_clusters)
        elif model_choice == "DBSCAN":
            mlflow.log_param("eps", eps)
            mlflow.log_param("min_samples", min_samples)

        st.write("‚è≥ ƒêang hu·∫•n luy·ªán m√¥ h√¨nh...")
        model.fit(X_train_reduced)
        labels = model.labels_

        # T√≠nh to√°n silhouette score
        if len(np.unique(labels)) > 1:
            silhouette_avg = silhouette_score(X_train_reduced, labels)
            st.success(f"üìä **Silhouette Score**: {silhouette_avg:.4f}")
            mlflow.log_metric("silhouette_score", silhouette_avg)
        else:
            st.warning("‚ö† Kh√¥ng th·ªÉ t√≠nh silhouette score v√¨ ch·ªâ c√≥ m·ªôt c·ª•m.")

        # L∆∞u m√¥ h√¨nh v√†o session_state
        if "models" not in st.session_state:
            st.session_state["models"] = []

        model_name = model_choice.lower().replace(" ", "_")
        if model_choice == "DBSCAN":
            model_name += f"_eps{eps}_min_samples{min_samples}"
        elif model_choice == "K-means":
            model_name += f"_n_clusters{n_clusters}"

        existing_model = next((item for item in st.session_state["models"] if item["name"] == model_name), None)

        if existing_model:
            count = 1
            new_model_name = f"{model_name}_{count}"
            while any(item["name"] == new_model_name for item in st.session_state["models"]):
                count += 1
                new_model_name = f"{model_name}_{count}"
            model_name = new_model_name
            st.warning(f"‚ö†Ô∏è M√¥ h√¨nh ƒë∆∞·ª£c l∆∞u v·ªõi t√™n: {model_name}")

        st.session_state["models"].append({"name": model_name, "model": model})
        st.write(f"üîπ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u v·ªõi t√™n: {model_name}")
        st.write(f"T·ªïng s·ªë m√¥ h√¨nh hi·ªán t·∫°i: {len(st.session_state['models'])}")

        st.write("üìã Danh s√°ch c√°c m√¥ h√¨nh ƒë√£ l∆∞u:")
        model_names = [model["name"] for model in st.session_state["models"]]
        st.write(", ".join(model_names))

        st.success(f"‚úÖ ƒê√£ log d·ªØ li·ªáu cho **Train_{st.session_state['run_name']}**!")
        st.markdown(f"üîó [Truy c·∫≠p MLflow UI]({st.session_state['mlflow_url']})")


def train():
    mlflow_input()
    if "X_train" in st.session_state:
        X_train = st.session_state.X_train 
        X_val = st.session_state.X_val 
        X_test = st.session_state.X_test 
        y_train = st.session_state.y_train 
        y_val = st.session_state.y_val 
        y_test = st.session_state.y_test 
    else:
        st.error("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu! H√£y chia d·ªØ li·ªáu tr∆∞·ªõc.")
        return

    X_train = X_train.reshape(-1, 28 * 28) / 255.0
    X_test = X_test.reshape(-1, 28 * 28) / 255.0

    st.header("‚öôÔ∏è Ch·ªçn m√¥ h√¨nh & Hu·∫•n luy·ªán")

    model_choice = st.selectbox(
        "Ch·ªçn m√¥ h√¨nh:", 
        ["K-means", "DBSCAN"], 
        key="clustering_model_choice_selectbox"  # Th√™m key duy nh·∫•t
    )

    # Gi·∫£m chi·ªÅu d·ªØ li·ªáu
    reduction_method = st.selectbox(
        "Ch·ªçn ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu d·ªØ li·ªáu:", 
        ["PCA", "t-SNE"], 
        key="clustering_reduction_method_selectbox"  # Th√™m key duy nh·∫•t
    )
    n_components = st.slider(
        "S·ªë chi·ªÅu sau khi gi·∫£m:", 
        2, 
        50 if reduction_method == "PCA" else 3,  # Gi·ªõi h·∫°n t-SNE t·ªëi ƒëa l√† 3
        2,
        key="clustering_n_components_slider"  # Th√™m key duy nh·∫•t
    )

    # L∆∞u v√†o session_state
    st.session_state.reduction_method = reduction_method
    st.session_state.n_components = n_components

    X_train_reduced = reduce_dimensions(X_train, method=reduction_method, n_components=n_components)
    X_test_reduced = reduce_dimensions(X_test, method=reduction_method, n_components=n_components)

    if model_choice == "K-means":
        st.markdown("""
        - **K-means** l√† m·ªôt thu·∫≠t to√°n ph√¢n c·ª•m d·ª±a tr√™n kho·∫£ng c√°ch gi·ªØa c√°c ƒëi·ªÉm d·ªØ li·ªáu.
        - **Tham s·ªë c·∫ßn ch·ªçn:**  
            - **n_clusters**: S·ªë l∆∞·ª£ng c·ª•m.  
        """)
        n_clusters = st.slider(
            "n_clusters", 
            2, 20, 10, 
            key="clustering_n_clusters_slider"  # Th√™m key duy nh·∫•t
        )
        model = KMeans(n_clusters=n_clusters)

    elif model_choice == "DBSCAN":
        st.markdown("""
        - **DBSCAN** l√† m·ªôt thu·∫≠t to√°n ph√¢n c·ª•m d·ª±a tr√™n m·∫≠t ƒë·ªô.
        """)
        eps = st.slider(
            "eps (Kho·∫£ng c√°ch t·ªëi ƒëa gi·ªØa hai ƒëi·ªÉm ƒë·ªÉ coi l√† l√¢n c·∫≠n)", 
            0.1, 1.0, 0.5, 
            key="clustering_eps_slider"  # Th√™m key duy nh·∫•t
        )
        min_samples = st.slider(
            "min_samples (S·ªë l∆∞·ª£ng ƒëi·ªÉm t·ªëi thi·ªÉu trong m·ªôt l√¢n c·∫≠n)", 
            1, 20, 5, 
            key="clustering_min_samples_slider"  # Th√™m key duy nh·∫•t
        )
        model = DBSCAN(eps=eps, min_samples=min_samples)

    if st.button("Hu·∫•n luy·ªán m√¥ h√¨nh", key="clustering_train_button"):  # Th√™m key duy nh·∫•t
        st.session_state["run_name"] = "training_run_1"  # T·∫°o gi√° tr·ªã cho run_name

        # G·ªçi h√†m hu·∫•n luy·ªán m√¥ h√¨nh
        train_model(
            model=model,
            X_train_reduced=X_train_reduced,
            y_train=y_train,
            X_test_reduced=X_test_reduced,
            y_test=y_test,
            model_choice=model_choice,
            reduction_method=reduction_method,
            n_components=n_components,
            n_clusters=n_clusters if model_choice == "K-means" else None,
            eps=eps if model_choice == "DBSCAN" else None,
            min_samples=min_samples if model_choice == "DBSCAN" else None
        )


def du_doan():
    st.title("üî¢ D·ª± ƒëo√°n ch·ªØ s·ªë vi·∫øt tay")

    if "models" not in st.session_state or not st.session_state["models"]:
        st.error("‚ö†Ô∏è Ch∆∞a c√≥ m√¥ h√¨nh n√†o ƒë∆∞·ª£c hu·∫•n luy·ªán. H√£y hu·∫•n luy·ªán m√¥ h√¨nh tr∆∞·ªõc.")
        return

    model_names = [model["name"] for model in st.session_state["models"]]
    selected_model_name = st.selectbox(
        "üîç Ch·ªçn m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán:", 
        model_names, 
        key="clustering_model_selectbox"  # Th√™m key duy nh·∫•t
    )
    selected_model = next(model["model"] for model in st.session_state["models"] if model["name"] == selected_model_name)

    uploaded_file = st.file_uploader(
        "üì§ T·∫£i l√™n ·∫£nh ch·ªØ s·ªë vi·∫øt tay (28x28 pixel)", 
        type=["png", "jpg", "jpeg"], 
        key="clustering_file_uploader"  # Th√™m key duy nh·∫•t
    )

    if uploaded_file is not None:
        image = Image.open(uploaded_file).convert("L")
        image = ImageOps.invert(image)
        image = image.resize((28, 28))
        st.image(image, caption="·∫¢nh ƒë√£ t·∫£i l√™n", use_column_width=True)

        img_array = np.array(image).reshape(1, -1) / 255.0
        prediction = selected_model.predict(img_array)
        st.success(f"üî¢ D·ª± ƒëo√°n: **{prediction[0]}**")

def show_experiment_selector():
    st.title("üìä MLflow Experiments")

    experiment_name = "Clustering"
    
    experiments = mlflow.search_experiments()
    selected_experiment = next((exp for exp in experiments if exp.name == experiment_name), None)

    if not selected_experiment:
        st.error(f"‚ùå Experiment '{experiment_name}' kh√¥ng t·ªìn t·∫°i!")
        return

    st.subheader(f"üìå Experiment: {experiment_name}")
    st.write(f"**Experiment ID:** {selected_experiment.experiment_id}")
    st.write(f"**Tr·∫°ng th√°i:** {'Active' if selected_experiment.lifecycle_stage == 'active' else 'Deleted'}")
    st.write(f"**V·ªã tr√≠ l∆∞u tr·ªØ:** {selected_experiment.artifact_location}")

    runs = mlflow.search_runs(experiment_ids=[selected_experiment.experiment_id])

    if runs.empty:
        st.warning("‚ö† Kh√¥ng c√≥ runs n√†o trong experiment n√†y.")
        return

    st.write("### üèÉ‚Äç‚ôÇÔ∏è C√°c Runs g·∫ßn ƒë√¢y:")

    run_info = []
    for _, run in runs.iterrows():
        run_id = run["run_id"]
        run_params = mlflow.get_run(run_id).data.params
        run_name = run_params.get("run_name", f"Run {run_id[:8]}")
        run_info.append((run_name, run_id))

    run_name_to_id = dict(run_info)
    run_names = list(run_name_to_id.keys())

    # Th√™m key duy nh·∫•t cho selectbox
    selected_run_name = st.selectbox(
        "üîç Ch·ªçn m·ªôt run:", 
        run_names, 
        key="clustering_experiment_selectbox"  # Th√™m key duy nh·∫•t
    )
    selected_run_id = run_name_to_id[selected_run_name]

    selected_run = mlflow.get_run(selected_run_id)

    if selected_run:
        st.subheader(f"üìå Th√¥ng tin Run: {selected_run_name}")
        st.write(f"**Run ID:** {selected_run_id}")
        st.write(f"**Tr·∫°ng th√°i:** {selected_run.info.status}")
        start_time_ms = selected_run.info.start_time

        if start_time_ms:
            start_time = datetime.fromtimestamp(start_time_ms / 1000).strftime("%Y-%m-%d %H:%M:%S")
        else:
            start_time = "Kh√¥ng c√≥ th√¥ng tin"

        st.write(f"**Th·ªùi gian ch·∫°y:** {start_time}")

        params = selected_run.data.params
        metrics = selected_run.data.metrics

        if params:
            st.write("### ‚öôÔ∏è Parameters:")
            st.json(params)

        if metrics:
            st.write("### üìä Metrics:")
            st.json(metrics)

        dataset_path = f"{selected_experiment.artifact_location}/{selected_run_id}/artifacts/dataset.csv"
        st.write("### üìÇ Dataset:")
        st.write(f"üì• [T·∫£i dataset]({dataset_path})")
    else:
        st.warning("‚ö† Kh√¥ng t√¨m th·∫•y th√¥ng tin cho run n√†y.")


def Clusterting():
    st.title(" MNIST Clustering App")

    tab1, tab2, tab3, tab4 = st.tabs(["üìò Data", "‚öôÔ∏è Hu·∫•n luy·ªán", "üî¢ D·ª± ƒëo√°n", "üî•Mlflow"])

    with tab1:
        data()
        
    with tab2:
        split_data()
        train()
        
    with tab3:
        du_doan()   
    with tab4:
        show_experiment_selector()  

if __name__ == "__main__":
    Clusterting()
